{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNG+8qO8uPKYyGEMhHqFRxY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SameulAH/Brain-MRI-Segmentation/blob/main/Brain_MRI_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xU72UDKBdi8u"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchview"
      ],
      "metadata": {
        "id": "IirVNZd1dn0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from skimage.transform import rotate\n",
        "from skimage.util import montage\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "import cv2\n",
        "import tensorflow\n",
        "import random\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "import numpy as np\n",
        "from keras.callbacks import CSVLogger\n",
        "import keras.backend as K\n",
        "import zipfile\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "LV4AeaxReiGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brats2020 Dataset"
      ],
      "metadata": {
        "id": "Up2dp3hivvBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the .kaggle directory and move the kaggle.json file there\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the dataset\n",
        "!kaggle datasets download -d awsaf49/brats20-dataset-training-validation\n"
      ],
      "metadata": {
        "id": "crhDjbQOllAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip dataset\n",
        "path_to_zip_file = \"brats20-dataset-training-validation.zip\"\n",
        "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"brats20-dataset-training-validation\")"
      ],
      "metadata": {
        "id": "sp19ACzKqw7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a small error in one file"
      ],
      "metadata": {
        "id": "JlenbEMAv6rN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Absolute path of the incorrectly named file\n",
        "old_name = \"brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_355/W39_1998.09.19_Segm.nii\"\n",
        "new_name = \"brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_355/BraTS20_Training_355_seg.nii\"\n",
        "\n",
        "# Renaming the file\n",
        "try:\n",
        "    os.rename(old_name, new_name)\n",
        "    print(\"The file has been successfully renamed\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Maybe you have already renamed the file or the file is not misspelled on the dataset anymore\")"
      ],
      "metadata": {
        "id": "IFwAVgtnv_y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EDA**"
      ],
      "metadata": {
        "id": "CvHLijXwwCrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* The ground truth segmentation of the tumoral and non-tumoral regions of their brains, which has been manually realised by experts.\n",
        "\n",
        "\n",
        "**  We have 4 different types of magnetic resonance imaging (MRI) scans of their brain, also known as modalities, which are named T1, T1ce, T2 and FLAIR.**\n"
      ],
      "metadata": {
        "id": "STJLXhovwGkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* T1-weighted MRI (T1): Provides high-resolution images of brain anatomy, excellent for visualizing normal structures and detecting lesions.\n",
        "* T1-weighted contrast-enhanced MRI (T1ce): Similar to T1, but with a contrast agent to enhance visibility of tumors and areas with disrupted blood-brain barrier.\n",
        "* T2-weighted MRI (T2):  Sensitive to fluid, highlighting edema and inflammation, making it useful for detecting lesions and tumors.\n",
        "* FLAIR (Fluid-Attenuated Inversion Recovery): A T2-weighted sequence that suppresses cerebrospinal fluid signals, enhancing visibility of lesions near fluid spaces.\n",
        "\n"
      ],
      "metadata": {
        "id": "-m74h8wLw8Ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify a sample path (here we will take the first patient of the Training dataset)\n",
        "sample_path = 'brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_'\n",
        "\n",
        "# Load the 4 MRI modalities and the segmentation located in the patient's path using the nibabel library\n",
        "t1_img=nib.load(sample_path + 't1.nii')\n",
        "t1ce_img=nib.load(sample_path + 't1ce.nii')\n",
        "t2_img=nib.load(sample_path + 't2.nii')\n",
        "flair_img=nib.load(sample_path + 'flair.nii')\n",
        "seg_img=nib.load(sample_path + 'seg.nii')\n",
        "\n",
        "# Get the image data\n",
        "t1_data = t1_img.get_fdata()\n",
        "t1ce_data = t1ce_img.get_fdata()\n",
        "t2_data = t2_img.get_fdata()\n",
        "flair_data = flair_img.get_fdata()\n",
        "seg_data = seg_img.get_fdata()\n",
        "\n",
        "# Plot the 101th slice of the 4 RMI modalities and the segmentation\n",
        "slice_nb = 101\n",
        "\n",
        "fig, axs = plt.subplots(1, 5, figsize=(20,20))\n",
        "axs[0].imshow(t1_data[:,:,slice_nb], cmap=\"gray\")\n",
        "axs[0].set_title('T1')\n",
        "axs[1].imshow(t1ce_data[:,:,slice_nb], cmap=\"gray\")\n",
        "axs[1].set_title('T1CE')\n",
        "axs[2].imshow(t2_data[:,:,slice_nb], cmap=\"gray\")\n",
        "axs[2].set_title('T2')\n",
        "axs[3].imshow(flair_data[:,:,slice_nb], cmap=\"gray\")\n",
        "axs[3].set_title('FLAIR')\n",
        "axs[4].imshow(seg_data[:,:,slice_nb], cmap=\"gray\")\n",
        "axs[4].set_title('Segmentation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9806TkfKwDTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For an expert, it can be useful to have these 4 modalities in order to analyze the tumor more precisely, and to confirm its presence or not.\n",
        "\n",
        "But for our artificial approach, using only two modalities instead of four is interesting since it can reduce the computational and memory requirements of the segmentation task, making it faster and more efficient. That is why we will exclude T1, since we have its improved version T1ce. Also, we will exclude the T2 modality. Indeed, the fluids it presents could degrade our predictions. These fluids are removed in the flair version, which highlights the affected regions much better, and will therefore be much more interesting for our training."
      ],
      "metadata": {
        "id": "X4Omwct2y2xJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Images format"
      ],
      "metadata": {
        "id": "LAEkUCJazpdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can notice that these images are in .nii format. Indeed, these scans are NIfTI files (Neuroimaging Informatics Technology Initiative). A NIfTI image is a digital representation of a 3D object, such as a brain in our case."
      ],
      "metadata": {
        "id": "Q93XsmNJzreU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Modality shape\n",
        "print(t1_data.shape)\n",
        "\n",
        "# Segmentation shape\n",
        "print(seg_data.shape)"
      ],
      "metadata": {
        "id": "tftTd4gDwZNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a RMI modality through all planes\n",
        "slice_nb = 100\n",
        "\n",
        "fig, axs2 = plt.subplots(1, 3, figsize=(10,10))\n",
        "\n",
        "# Apply a 90Â° rotation with an automatic resizing, otherwise the display is less obvious to analyze\n",
        "axs2[0].imshow(rotate(t1_data[slice_nb,:,:], 90, resize=True), cmap=\"gray\")\n",
        "axs2[0].set_title('T1 - Sagittal View')\n",
        "\n",
        "axs2[1].imshow(rotate(t1_data[:,slice_nb,:], 90, resize=True), cmap=\"gray\")\n",
        "axs2[1].set_title('T1 - Coronal View')\n",
        "\n",
        "axs2[2].imshow(t1_data[:,:,slice_nb], cmap=\"gray\")\n",
        "axs2[2].set_title('T1 - Axial View')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zkFtfUso1m4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a RMI modality through all planes\n",
        "slice_nb = 100\n",
        "\n",
        "fig, axs2 = plt.subplots(1, 3, figsize=(10,10))\n",
        "\n",
        "# Apply a 90Â° rotation with an automatic resizing, otherwise the display is less obvious to analyze\n",
        "axs2[0].imshow(rotate(t2_data[slice_nb,:,:], 90, resize=True), cmap=\"gray\")\n",
        "axs2[0].set_title('T2 - Sagittal View')\n",
        "\n",
        "axs2[1].imshow(rotate(t2_data[:,slice_nb,:], 90, resize=True), cmap=\"gray\")\n",
        "axs2[1].set_title('T2 - Coronal View')\n",
        "\n",
        "axs2[2].imshow(t2_data[:,:,slice_nb], cmap=\"gray\")\n",
        "axs2[2].set_title('T2 - Axial View')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DX-svZvo1noV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(1, 1, figsize = (15,15))\n",
        "ax1.imshow(rotate(montage(t1_data[:,:,:]), 90, resize=True), cmap ='gray')\n"
      ],
      "metadata": {
        "id": "Aj_PPd9O1y52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a segmantation\n",
        "some_seg_img = nib.load(\"brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_seg.nii\").get_fdata()\n",
        "\n",
        "cmap = mpl.colors.ListedColormap(['#440054', '#3b528b', '#18b880', '#e6d74f'])\n",
        "norm = mpl.colors.BoundaryNorm([-0.5, 0.5, 1.5, 2.5, 3.5], cmap.N)\n",
        "\n",
        "plt.imshow(some_seg_img[100,:,:], cmap=cmap, norm=norm)\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "RvgBtTyY2lDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " you will notice that some slices have multiple colors (here 4 colors), which means that the experts have assigned multiple values to the segmentation"
      ],
      "metadata": {
        "id": "sR5bg4rJ4YrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\n",
        "seg_samples = [os.path.join(data_path, sample, f\"{sample}_seg.nii\") for sample in os.listdir(data_path) if not sample.endswith('.csv')]\n",
        "\n",
        "saved_values = []\n",
        "max_nb_values = 0\n",
        "for sample in seg_samples:\n",
        "    seg_img = nib.load(sample).get_fdata()\n",
        "    unique_values = np.unique(seg_img)\n",
        "    nb_unique_values = len(np.unique(seg_img))\n",
        "\n",
        "    if nb_unique_values > max_nb_values:\n",
        "        max_nb_values = nb_unique_values\n",
        "        saved_values = unique_values\n",
        "\n",
        "print(f\"Maximum number of values in all segmentation images: {max_nb_values}\")\n",
        "print(f\"Values: {saved_values}\")"
      ],
      "metadata": {
        "id": "F2RlUaOk5v7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have 4 possible values in the segmentation files. These 4 values will form our 4 classes. Here is what they correspond to:\n",
        "\n",
        "0 : Not Tumor (NT) which means Healthy Zone or Background\n",
        "1 : Necrotic and Non-Enhancing Tumor (NCR + NET)\n",
        "2 : Peritumoral Edema (ED)\n",
        "4 : Enhancing Tumor (ET)"
      ],
      "metadata": {
        "id": "vfmAFhHX7L4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our goal is to predict and segment each of these 4 classes for new patients to find out whether or not they have a brain tumor and which areas are affected.**"
      ],
      "metadata": {
        "id": "VZmRrtTN7REk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution of these classes in the dataset"
      ],
      "metadata": {
        "id": "j9I6lRhU7XpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "values, counts = np.unique(some_seg_img, return_counts=True)\n",
        "print(counts)"
      ],
      "metadata": {
        "id": "FOj-vNw04X9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame from the unique values and counts\n",
        "data = pd.DataFrame({'Class Labels': values, 'Counts': counts})\n",
        "\n",
        "# Create a count plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(data=data, x='Class Labels', y='Counts', palette='Blues_d')\n",
        "plt.title('Class Distribution in Segmentation Image')\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fMovaUxmfI10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To summarize data exploration:\n",
        "\n",
        "* We have for each sample / patient 4 different modalities (T1, T1CE, T2 & FLAIR), accompanied by a segmentation that indicates tumor areas.\n",
        "\n",
        "* Modalities T1CE and FLAIR are the more interesting to keep, since these 2 provide complementary information about the anatomy and tissue contrast of the patientâs brain.\n",
        "\n",
        "* Each image is 3D, and can therefore be analyzed through 3 different planes that are composed of 2D slices.\n",
        "\n",
        "* Many slices contain little or no information. We will only keep the (60:135) slices interval for this tutorial. Of course, you are free to customize the code to send less or more slices to your model, but the training time will be longer.\n",
        "\n",
        "* A segmentation image contains 1 to 4 classes.\n",
        "\n",
        "* Class number 4 must be reassigned to 3 since value 3 is missing.\n",
        "\n",
        "* Class 0 (background) is over-represented in the majority of the scans. However, cropping can remove important information. We will not use it in this tutorial and will keep the images as they are."
      ],
      "metadata": {
        "id": "3en14OMD88kS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define selected slices range\n",
        "VOLUME_START_AT = 60\n",
        "VOLUME_SLICES = 75"
      ],
      "metadata": {
        "id": "Lr-4MdUk4E59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-Val split"
      ],
      "metadata": {
        "id": "MAFg0kmv9Yl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify path of our BraTS2020 directory\n",
        "data_path = \"brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\n",
        "\n",
        "# Retrieve all samples from path with listdir(). This method lists of all files + directories in the specified directory.\n",
        "samples = os.listdir(data_path)\n",
        "print(\"Number of samples:\", len(samples))"
      ],
      "metadata": {
        "id": "UNN_n6189hBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We obtain 371 files while the braTS dataset contains only 369 samples. The reason is simple: 2 .csv files are present in the MICCAI_BraTS2020_TrainingData directory"
      ],
      "metadata": {
        "id": "__HrOSiR-Foo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples.remove(\"survival_info.csv\")\n",
        "samples.remove(\"name_mapping.csv\")"
      ],
      "metadata": {
        "id": "tT25abbY9WfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and validation sets\n",
        "samples_train, samples_val = train_test_split(samples, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the train set into the real train set and in a test set\n",
        "samples_train, samples_test = train_test_split(samples_train, test_size=0.15, random_state=42)\n",
        "\n",
        "# Print data distribution (Train: 68%, Test: 12%, Val: 20%)\n",
        "print(f\"Train length: {len(samples_train)}\")\n",
        "print(f\"Validation length: {len(samples_val)}\")\n",
        "print(f\"Test length: {len(samples_test)}\")"
      ],
      "metadata": {
        "id": "2HhvtAkv-C3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Generator"
      ],
      "metadata": {
        "id": "gpm8dZHa-snv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to train a neural network to segment objects in images, it is necessary to feed it with both the raw image data (X) and the ground truth segmentations (y). By combining these two types of data, the neural network can learn to recognize tumor patterns and make accurate predictions about the contents of a patientâs scan.\n",
        "\n",
        "Unfortunately, our modalities images (X) and our segmentations (y) cannot be sent directly to the AI model. Indeed, loading all these 3D images would overload the memory of our environment and cause the system to crash. This will also lead to shape mismatch errors."
      ],
      "metadata": {
        "id": "SnYhmby7-xHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** We have to do some image preprocessing before, which will be done by using a Data Generator, where we will perform any operation that we think is necessary when loading the images.**"
      ],
      "metadata": {
        "id": "pJ0dOATI-4Ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import nibabel as nib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "IMG_SIZE = 128\n",
        "VOLUME_SLICES = 75  # Adjust based on your data\n",
        "VOLUME_START_AT = 60  # Starting slice index\n",
        "data_path = '/content/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'  # Set your data path here\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 2, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim # Resized image dimensions (128 x 128)\n",
        "        self.batch_size = batch_size #  Number of images to load each time\n",
        "        self.list_IDs = list_IDs # Patients IDs\n",
        "        self.n_channels = n_channels # Number of channels (T1CE + FLAIR)\n",
        "        self.shuffle = shuffle # Indicates if data is shuffled for each epoch\n",
        "        self.on_epoch_end() # Updates indexes after each epoch\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        Batch_ids = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Load & Generate data\n",
        "        X, y = self.__data_generation(Batch_ids)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, Batch_ids):\n",
        "        'Generates data containing batch_size samples'\n",
        "        # Initialization\n",
        "        X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n",
        "        y = np.zeros((self.batch_size*VOLUME_SLICES, 240, 240))\n",
        "\n",
        "        # Generate data\n",
        "        for c, i in enumerate(Batch_ids):\n",
        "\n",
        "            # Get path of each RMI modality and the segmentation\n",
        "            sample_path = os.path.join(data_path, i, i)\n",
        "            t1ce_path = sample_path + '_t1ce.nii'\n",
        "            flair_path = sample_path + '_flair.nii'\n",
        "            seg_path = sample_path + '_seg.nii'\n",
        "            #t1_path = sample_path + '_t1.nii'\n",
        "            #t2_path = sample_path + '_t2.nii'\n",
        "\n",
        "            # Extract the data from these paths\n",
        "            t1ce = nib.load(t1ce_path).get_fdata()\n",
        "            flair = nib.load(flair_path).get_fdata()\n",
        "            seg = nib.load(seg_path).get_fdata()\n",
        "            #t1 = nib.load(t1_paths).get_fdata()\n",
        "            #t2 = nib.load(t2_path).get_fdata()\n",
        "\n",
        "            for j in range(VOLUME_SLICES):\n",
        "                 X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))\n",
        "                 X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(t1ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "                 y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT]\n",
        "\n",
        "        # Masks / Segmentations\n",
        "        y[y==4] = 3\n",
        "        mask = tensorflow.one_hot(y, 4)\n",
        "        Y = tensorflow.image.resize(mask, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "\n",
        "        print(\"X shape:\", X.shape)\n",
        "        print(\"Y shape:\", Y.shape)\n",
        "\n",
        "\n",
        "        return X/np.max(X), Y\n",
        "\n",
        "\n",
        "# Assuming samples_train, samples_val, and samples_test are defined and contain your IDs\n",
        "training_generator = DataGenerator(samples_train, batch_size=1)  # Set your desired batch size\n",
        "valid_generator = DataGenerator(samples_val, batch_size=1)\n",
        "test_generator = DataGenerator(samples_test, batch_size=1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zo_oNgZw-MvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To summarize the preprocessing steps:\n",
        "\n",
        "* We use a data generator to be able to process and send our data to our neural network (since all our images cannot be stored in memory at once).\n",
        "* For each epoch (single pass of the entire training dataset through a neural network), the model will receive 250 samples (those contained in our training dataset).\n",
        "* For each sample, the model will have to analyze 150 slices (since there are two modalities, and 75 selected slices for both of them), received in a (128, 128) shape, as an X array of a (128, 128, 75, 2) shape. This array will be provided with the ground truth segmentation of the patient, which will be One-Hot encoded and will then have a (75, 128, 128, 4) shape."
      ],
      "metadata": {
        "id": "IU4n0HNxCkwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = training_generator[0]  # Test if this works and outputs valid data\n",
        "print(X.shape, y.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "zybKaGlFWM7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The shapes of X and y indicate that your DataGenerator is correctly outputting data with the expected dimensions:\n",
        "\n",
        "- X has the shape (75, 128, 128, 2), which corresponds to 75 slices of 128x128 images with 2 channels (T1CE and FLAIR).\n",
        "- y has the shape (75, 128, 128, 4), indicating 75 slices of 128x128 segmentation masks with 4 classes (one-hot encoded)."
      ],
      "metadata": {
        "id": "WH41XoPqWdy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X2_22rtOWbsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "sWMSA3EqCr_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## U-Net"
      ],
      "metadata": {
        "id": "XIydmKEYC4ZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# U-Net implementation for BraTS 2019 by Naomi Fridman, https://naomi-fridman.medium.com/multi-class-image-segmentation-a5cc671e647a\n",
        "def build_unet(inputs, ker_init, dropout):\n",
        "    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(inputs)\n",
        "    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv1)\n",
        "\n",
        "    pool = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool)\n",
        "    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n",
        "\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv2)\n",
        "\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv3)\n",
        "\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool4)\n",
        "    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv5)\n",
        "    drop5 = Dropout(dropout)(conv5)\n",
        "\n",
        "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = 2)(drop5))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge7)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv7)\n",
        "\n",
        "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = 2)(conv7))\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge8)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv8)\n",
        "\n",
        "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = 2)(conv8))\n",
        "    merge9 = concatenate([conv,up9], axis = 3)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge9)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv9)\n",
        "\n",
        "    up = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = 2)(conv9))\n",
        "    merge = concatenate([conv1,up], axis = 3)\n",
        "    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge)\n",
        "    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n",
        "\n",
        "    conv10 = Conv2D(4, 1, activation = 'softmax')(conv)\n",
        "\n",
        "    return Model(inputs = inputs, outputs = conv10)"
      ],
      "metadata": {
        "id": "NjiN0eBQBmLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Funtion"
      ],
      "metadata": {
        "id": "m6RoLRvGD0Be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute metric between the predicted segmentation and the ground truth\n",
        "def dice_coef(y_true, y_pred, smooth=1.0):\n",
        "    class_num = 4\n",
        "    for i in range(class_num):\n",
        "        y_true_f = K.flatten(y_true[:,:,:,i])\n",
        "        y_pred_f = K.flatten(y_pred[:,:,:,i])\n",
        "        intersection = K.sum(y_true_f * y_pred_f)\n",
        "        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
        "        if i == 0:\n",
        "            total_loss = loss\n",
        "        else:\n",
        "            total_loss = total_loss + loss\n",
        "    total_loss = total_loss / class_num\n",
        "    return total_loss\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "def sensitivity(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    return true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "\n",
        "def specificity(y_true, y_pred):\n",
        "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
        "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
        "    return true_negatives / (possible_negatives + K.epsilon())"
      ],
      "metadata": {
        "id": "y2E8TuRBD2Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will of course use accuracy, which is a very popular measure. However, this metric can be misleading when working with imbalanced datasets like BraTS2020, where Background class is over represented. To address this issue, we will use other metrics such as the intersection over union (IoU), the Dice coefficient, precision, sensitivity, and specificity.\n",
        "\n",
        "* Accuracy: Measures the overall proportion of correctly classified pixels, including both positive and negative pixels.\n",
        "\n",
        "* IoU: Measures the overlap between the predicted and ground truth segmentations.\n",
        "\n",
        "* Sensitivity (recall or true positive rate): Measures the proportion of positive ground truth pixels that were correctly predicted as positive.\n",
        "\n",
        "* Precision (positive predictive value): Measures the proportion of predicted positive pixels that are actually positive.\n",
        "\n",
        "* Specificity (true negative rate): Measures the proportion of negative ground truth pixels that were correctly predicted as negative."
      ],
      "metadata": {
        "id": "ZBt3sTEWE8zE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Build"
      ],
      "metadata": {
        "id": "z-U5QSbHD84p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate"
      ],
      "metadata": {
        "id": "BVvfnTAkEXuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input data shape\n",
        "input_layer = Input((IMG_SIZE, IMG_SIZE, 2))\n",
        "\n"
      ],
      "metadata": {
        "id": "NznS7Q2AD8Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and compile the model\n",
        "model = build_unet(input_layer, 'he_normal', 0.2)\n"
      ],
      "metadata": {
        "id": "o31AmGgpEm-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),  # Lower initial learning rate\n",
        "    metrics=['accuracy', tf.keras.metrics.MeanIoU(num_classes=4), dice_coef, precision, sensitivity, specificity]\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QsCxbaIFD3x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#def scheduler(epoch, lr):\n",
        "\n",
        "#    if epoch < 10:\n",
        "  #      return lr  # Keep the initial learning rate for the first 10 epochs\n",
        "   # else:\n",
        "    #    return lr * tf.math.exp(-0.1)  # Decay the learning rate exponentially\n",
        "\n",
        "#lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n"
      ],
      "metadata": {
        "id": "wolH2R9OJ8b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.000001, verbose=1),\n",
        "    keras.callbacks.ModelCheckpoint('model_{epoch:02d}-{val_accuracy:.6f}.weights.h5',\n",
        "                                    monitor='val_accuracy',\n",
        "                                    save_best_only=True,\n",
        "                                    save_weights_only = True,\n",
        "                                    mode='max',\n",
        "                                    verbose=1),\n",
        "    CSVLogger('training.log', separator=',', append=False)\n",
        "    # You can add more callbacks back one by one, such as CSVLogger or custom callbacks, to isolate the issue.\n",
        "]\n"
      ],
      "metadata": {
        "id": "r1PPNgWRKBJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ReduceLROnPlateau: This callback reduces the learning rate when a metric has stopped improving (validation loss here). The learning rate is reduced by a factor of 0.2, the patience is set to 2 epochs, and the minimum learning rate is set to 0.000001.\n",
        "\n",
        "* ModelCheckpoint: Saves the best model weights (model that has obtained the lowest validation loss during the different epochs). Saving a model allows us to reuse it later or to share it, without having to retrain it from scratch. This will save us time and resources!"
      ],
      "metadata": {
        "id": "wOGkiuJEFa23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Train & Save the model"
      ],
      "metadata": {
        "id": "e5nUEaaoFhJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del(model)\n",
        "#del(history)"
      ],
      "metadata": {
        "id": "icIuPtDneXpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras.api.layers import Dense\n",
        "from keras.api.models import Sequential\n",
        "from keras.api.optimizers import Adam\n",
        "# Calculate steps per epoch\n",
        "batch_size = 1\n",
        "steps_per_epoch = len(samples_train) // batch_size\n",
        "validation_steps =  len(samples_val) // batch_size\n",
        "\n",
        "\n",
        "history = model.fit(training_generator,validation_data= valid_generator, epochs=33,callbacks=callbacks, verbose=1)\n"
      ],
      "metadata": {
        "id": "I7j18lKsmAOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot accuracy, dice coefficient, precision, sensitivity, specificity, and loss\n",
        "\n",
        "plt.figure(figsize=(18, 6))  # Adjusted size for 4 plots in a row\n",
        "\n",
        "# Plot training & validation accuracy\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Plot training & validation dice coefficient\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.plot(history.history['dice_coef'], label='Training Dice Coefficient')\n",
        "plt.plot(history.history['val_dice_coef'], label='Validation Dice Coefficient')\n",
        "plt.title('Dice Coefficient')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Dice Coefficient')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Plot training & validation precision\n",
        "plt.subplot(1, 4, 3)\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.plot(history.history['mean_io_u_6'], label='Mean IOU')\n",
        "plt.plot(history.history['val_mean_io_u_6'], label='Validation Mean IOU')\n",
        "plt.title('Mean IOU')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean IOU')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "D0xbUuptWMBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the accuracy graph, we can see that both training accuracy and validation accuracy are increasing over epochs and reaching a plateau. This indicates that the model is learning from the data and generalizing well to new one.\n",
        "\n",
        "Then, we can see that our models is clearly learning from the training data, since both losses decrease over time on the second graph."
      ],
      "metadata": {
        "id": "u7jyaIccooVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict tumor segmentations**"
      ],
      "metadata": {
        "id": "88U6Sd4SovT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the model is trained, it's time to use it to predict the segmentations in our test dataset!"
      ],
      "metadata": {
        "id": "2aCT79pOqMag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Use the best model's weights (from epoch 29)\n",
        "* Use the final model's weights"
      ],
      "metadata": {
        "id": "XJTB1sW7qSw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile a model and load our saved weights\n",
        "# IMG_SIZE = 128\n",
        "input_layer = Input((IMG_SIZE, IMG_SIZE, 2))\n",
        "\n",
        "best_saved_model = build_unet(input_layer, 'he_normal', 0.2)\n",
        "\n",
        "best_saved_model.compile(loss=\"categorical_crossentropy\", optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.001), metrics = ['accuracy',tensorflow.keras.metrics.MeanIoU(num_classes=4), dice_coef, precision, sensitivity, specificity] )\n",
        "\n",
        "best_saved_model.load_weights('model_29-0.991782.weights.h5')"
      ],
      "metadata": {
        "id": "k7PKZ2Z-p3-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_segmentation(sample_path):\n",
        "    # Load NIfTI (.nii) files of the sample (patient)\n",
        "    t1ce_path = sample_path + '_t1ce.nii'\n",
        "    flair_path = sample_path + '_flair.nii'\n",
        "    #t1_path = sample_path + '_t1.nii'\n",
        "    #t2_path = sample_path + '_t2.nii'\n",
        "\n",
        "    # Extract the data from these paths\n",
        "    t1ce = nib.load(t1ce_path).get_fdata()\n",
        "    flair = nib.load(flair_path).get_fdata()\n",
        "\n",
        "    # Create an empty array\n",
        "    X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2))\n",
        "\n",
        "    # Perform the same operations as our DataGenerator, to keep the same input shape\n",
        "    for j in range(VOLUME_SLICES):\n",
        "        X[j,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n",
        "        X[j,:,:,1] = cv2.resize(t1ce[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n",
        "\n",
        "    # Send our images to the CNN model and return predicted segmentation\n",
        "    return model.predict(X/np.max(X), verbose=1)"
      ],
      "metadata": {
        "id": "0BjC0wjoqEuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_predicted_segmentations(samples_list, slice_to_plot, cmap, norm):\n",
        "    # Choose a random patient\n",
        "    random_sample = random.choice(samples_list)\n",
        "\n",
        "    # Get path of this patient\n",
        "    random_sample_path = os.path.join(data_path, random_sample, random_sample)\n",
        "\n",
        "    # Predict patient's segmentation\n",
        "    predicted_seg = predict_segmentation(random_sample_path)\n",
        "\n",
        "    # Load patient's original segmentation (Ground truth)\n",
        "    seg_path = random_sample_path + '_seg.nii'\n",
        "    seg = nib.load(seg_path).get_fdata()\n",
        "\n",
        "    # Resize original segmentation to the same dimensions of the predictions. (Add VOLUME_START_AT because original segmentation contains 155 slices vs only 75 for our prediction)\n",
        "    seg=cv2.resize(seg[:,:,slice_to_plot+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_NEAREST)\n",
        "\n",
        "    # Differentiate segmentations by their labels\n",
        "    all = predicted_seg[slice_to_plot,:,:,1:4] # Deletion of class 0 (Keep only Core + Edema + Enhancing classes)\n",
        "    zero = predicted_seg[slice_to_plot,:,:,0] # Isolation of class 0, Background (kind of useless, it is the opposite of the \"all\")\n",
        "    first = predicted_seg[slice_to_plot,:,:,1] # Isolation of class 1, Core\n",
        "    second = predicted_seg[slice_to_plot,:,:,2] # Isolation of class 2, Edema\n",
        "    third = predicted_seg[slice_to_plot,:,:,3] # Isolation of class 3, Enhancing\n",
        "\n",
        "    # Plot Original segmentation & predicted segmentation\n",
        "    print(\"Patient number: \", random_sample)\n",
        "    fig, axstest = plt.subplots(1, 6, figsize=(25, 20))\n",
        "\n",
        "    # Original segmentation\n",
        "    axstest[0].imshow(seg, cmap, norm)\n",
        "    axstest[0].set_title('Original Segmentation')\n",
        "\n",
        "    # Layers 1, 2, 3\n",
        "    axstest[1].imshow(all)\n",
        "    axstest[1].set_title('Predicted Segmentation - all layers')\n",
        "\n",
        "    # Layer 0\n",
        "    axstest[2].imshow(zero)\n",
        "    axstest[2].set_title('Predicted Segmentation - layer 0')\n",
        "\n",
        "    # Layer 1\n",
        "    axstest[3].imshow(first)\n",
        "    axstest[3].set_title('Predicted Segmentation - layer 1')\n",
        "\n",
        "    # Layer 2\n",
        "    axstest[4].imshow(second)\n",
        "    axstest[4].set_title('Predicted Segmentation - layer 2')\n",
        "\n",
        "    # Layer 3\n",
        "    axstest[5].imshow(third)\n",
        "    axstest[5].set_title('Predicted Segmentation - layer 3')\n",
        "\n",
        "    # Add space between subplots\n",
        "    plt.subplots_adjust(wspace=0.8)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "v-u7PQI5qged"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot Random predictions & Compare with Original (Ground truth)**\n"
      ],
      "metadata": {
        "id": "z8T5cEB2qkb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_predicted_segmentations(samples_test, 60, cmap, norm)"
      ],
      "metadata": {
        "id": "kAfKo11EqjZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_predicted_segmentations(samples_test, 50, cmap, norm)"
      ],
      "metadata": {
        "id": "bkBzQ97zqjdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_predicted_segmentations(samples_test, 70, cmap, norm)"
      ],
      "metadata": {
        "id": "L9uVsw55qjjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "show_predicted_segmentations(samples_test, 70, cmap, norm)"
      ],
      "metadata": {
        "id": "F2VrpMejqjnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, predicted segmentations are quite accurate.\n",
        "\n",
        "Unfortunately, we have some false positives, which means that we detect a tumor when there is none originally present (cf 3rd plot, patient 266, slice 70)."
      ],
      "metadata": {
        "id": "spFz35t4rWqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Post-processing**"
      ],
      "metadata": {
        "id": "AcrFd9AKsb1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use the argmax decoding technique. This process consists in applying the argmax function to obtain a single label for each pixel, corresponding to the class with the highest probability. Indeed, our pixels currently have probability values for each class since they were obtained using the softmax activation function."
      ],
      "metadata": {
        "id": "S3bwfjWBsmLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_post_processed_segmentations(sample, slice_to_plot, cmap, norm):\n",
        "\n",
        "    # Get path of this patient\n",
        "    sample_path = os.path.join(data_path, sample, sample)\n",
        "\n",
        "    # Predict patient's segmentation\n",
        "    predicted_seg = predict_segmentation(sample_path)\n",
        "\n",
        "    # Load patient's original segmentation (Ground truth)\n",
        "    seg_path = sample_path + '_seg.nii'\n",
        "    seg = nib.load(seg_path).get_fdata()\n",
        "\n",
        "    # Resize original segmentation to the same dimensions of the predictions. (Add VOLUME_START_AT because original segmentation contains 155 slices vs only 75 for our prediction)\n",
        "    seg=cv2.resize(seg[:,:,slice_to_plot+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_NEAREST)\n",
        "\n",
        "    # Fix 4 to 3 to have the same values as in the predicted segmentation, and then same colors\n",
        "    seg[seg==4] = 3\n",
        "\n",
        "    # Remove background layer (0) from original segmentation\n",
        "    seg[seg==0] = np.nan\n",
        "\n",
        "    # Post-processing\n",
        "    # Get indexes for each class of the highest probability pixels. Array will then contain only [0 1 2 3] instead of probabilities\n",
        "    my_pred = np.argmax(predicted_seg, axis=3)\n",
        "    my_pred = my_pred[slice_to_plot, :, :]\n",
        "\n",
        "    # Remove background layer (0) from post-processed predicted segmentation\n",
        "    # To fix 0 to np.nan, we need to convert array as a float\n",
        "    my_pred = my_pred.astype(float)\n",
        "    my_pred[my_pred == 0] = np.nan\n",
        "\n",
        "    # Remove background layer (0) from classical predicted segmentation\n",
        "    all = predicted_seg[slice_to_plot,:,:,1:4]\n",
        "\n",
        "    # Plot Original segmentation & predicted segmentation without processing & predicted segmentation\n",
        "    print(\"Patient number: \", sample)\n",
        "    fig, axstest = plt.subplots(1, 3, figsize=(15, 10))\n",
        "\n",
        "    axstest[0].imshow(seg, cmap, norm)\n",
        "    axstest[0].set_title('Original Segmentation')\n",
        "\n",
        "    axstest[1].imshow(all)\n",
        "    axstest[1].set_title('Prediction (w/o post processing (layer 1,2,3)')\n",
        "\n",
        "    axstest[2].imshow(my_pred, cmap, norm)\n",
        "    axstest[2].set_title('Prediction (w/ post processing (layer 1,2,3)')\n",
        "\n",
        "    # Add space between subplots\n",
        "    plt.subplots_adjust(wspace=0.8)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mxzKLDIjrWLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_post_processed_segmentations(sample = \"BraTS20_Training_266\", slice_to_plot=60, cmap=cmap, norm=norm)"
      ],
      "metadata": {
        "id": "AiK95RL8rWQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_post_processed_segmentations(sample = \"BraTS20_Training_051\", slice_to_plot=50, cmap=cmap, norm=norm)"
      ],
      "metadata": {
        "id": "3a_jg7fcrWTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_post_processed_segmentations(sample = \"BraTS20_Training_266\", slice_to_plot=70, cmap=cmap, norm=norm)"
      ],
      "metadata": {
        "id": "9eMjh6sOGnoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_post_processed_segmentations(sample = \"BraTS20_Training_051\", slice_to_plot=70, cmap=cmap, norm=norm)"
      ],
      "metadata": {
        "id": "EQp51vaRp2zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_post_processed_segmentations(sample = \"BraTS20_Training_274\", slice_to_plot=70, cmap=cmap, norm=norm)"
      ],
      "metadata": {
        "id": "zGBCbWqhs7Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The argmax decoding technique seems to work pretty well! Indeed, the false positive (cf 3rd plot, patient_266, slice 70) was removed with post-processing and the other predictions were well preserved!"
      ],
      "metadata": {
        "id": "wR4xRzfItHfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate the model**"
      ],
      "metadata": {
        "id": "UwcS8W8qtQVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "\n",
        "results = model.evaluate(test_generator, batch_size=100, callbacks= callbacks)\n",
        "\n",
        "descriptions = [\"Loss\", \"Accuracy\", \"MeanIOU\", \"Dice coefficient\", \"Precision\", \"Sensitivity\", \"Specificity\"]\n",
        "\n",
        "# Combine results list and descriptions list\n",
        "results_list = zip(results, descriptions)\n",
        "\n",
        "# Display each metric with its description\n",
        "print(\"\\nModel evaluation on the test set:\")\n",
        "print(\"==================================\")\n",
        "for i, (metric, description) in enumerate(results_list):\n",
        "    print(f\"{description} : {round(metric, 4)}\")"
      ],
      "metadata": {
        "id": "dO5og6IAs_mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can conclude that the model performed very well on the test dataset, achieving a low test loss (0.0235), a correct dice coefficient (0.57) for an image segmentation task, and good scores on other metrics which indicate that the model has good generalization performance on unseen data."
      ],
      "metadata": {
        "id": "pgzrV_vCtfwG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v8u2GDoctWc3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}